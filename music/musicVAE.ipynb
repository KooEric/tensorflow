{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYSQe04s3yqlegj9VPOp/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KooEric/tensorflow/blob/main/musicVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 진행사항(From github)\n",
        "1. 자체 데이터로 모델을 학습시키려면 먼저 Magenta 환경을 설정\n",
        "2. 다음으로, 데이터세트 만들기의 지침에 따라 MIDI 파일 모음을 NoteSequences의 TFRecord로 변환  \n",
        "3. configs.py 에서 미리 정의된 구성 중 하나를 선택 하거나 직접 정의\n",
        "4. 마지막으로 training 스크립트 를 실행 \n",
        "> 해당 과정(전처리 -> 학습 -> 생성)의 프로세스"
      ],
      "metadata": {
        "id": "uDMa_LeSp0gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE는 AE와 형태 면에서 유사하며, 학습 데이터(training data)를 병목(Bottleneck) 구조를 통해서 압축하여 저차원의 잠재 코드(latent code, z)를 만듭니다. 잠재 공간(latent space)을 통해서 모델은 입력 데이터 간의 유사성과 차이를 학습합니다.\n",
        "\n",
        "VAE의 인코더는 입력 데이터를 평균과 표준 편차 벡터로 인코딩 한 후, 두 벡터에 대응하는 분포에서 샘플링을 수행합니다. 그리고 쿨백-라이블러 발산(KL divergence)을 손실함수로 사용하여, 해당 분포가 표준 정규 분포에 가까워지도록 학습하게 됩니다. 결국, 표준 정규 분포에 근접한 분포에서 샘플링된 VAE의 잠재 공간 분포는 원점을 기준으로 데이터가 대칭적이고, 분포가 고른 형태를 보이게 됩니다.\n",
        "\n",
        "Variational AutoEncoder(변분추론 인코더)의 경우 입력데이터 X를 잘 설명하는 특징(feature)을 추출하여 Latent vector(잠재백터) Z에 담은후에, 이 Latent vector(잠재백터) Z를 통하여, X와 유사하나 완전 새로운 데이터를 생성하는것을 목표로합니다."
      ],
      "metadata": {
        "id": "2DyA5r5vnt-9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YM7OWQZuntqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4cCfhbaQpSYr",
        "outputId": "e4ce6b65-5cf5-445a-e931-ded0f777419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting magenta==2.1.0\n",
            "  Downloading magenta-2.1.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 24.0 MB/s \n",
            "\u001b[?25hCollecting pretty-midi>=0.2.6\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (0.8.1)\n",
            "Collecting mir-eval>=0.4\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (0.16.0)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting note-seq\n",
            "  Downloading note_seq-0.0.5-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (1.15.0)\n",
            "Collecting python-rtmidi<1.2,>=1.1\n",
            "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (1.7.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (4.6.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (2.9.0)\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 51.6 MB/s \n",
            "\u001b[?25hCollecting numba<0.50\n",
            "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 69.7 MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]>=2.14.0\n",
            "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 67.7 MB/s \n",
            "\u001b[?25hCollecting sox>=1.3.7\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting mido==1.2.6\n",
            "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting sk-video\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (0.37.1)\n",
            "Requirement already satisfied: Pillow>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (7.1.2)\n",
            "Collecting pygtrie>=2.3\n",
            "  Downloading pygtrie-2.5.0.tar.gz (39 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.0) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.8.2)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.7)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.1.1)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.1.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: grpcio<2,>=1.33.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.47.0)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (6.0.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.2.1)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 925 kB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.35.0)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.2-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 66.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
            "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 76.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.0.3)\n",
            "Collecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core!=2.8.2,<3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.31.6)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.8.1-py2.py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 76.8 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
            "  Downloading google_cloud_pubsublite-1.4.3-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[K     |████████████████████████████████| 267 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigquery-storage<2.14,>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.8.0)\n",
            "Collecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.2.4)\n",
            "Collecting grpcio-gcp<1,>=0.2.2\n",
            "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0\n",
            "  Downloading google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n",
            "\u001b[K     |████████████████████████████████| 255 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.21.0)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.13.6-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 75.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.8.2,<3->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.56.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.8.2,<3->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.8.2,<3->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (57.4.0)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.9)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.1)\n",
            "Collecting protobuf<4,>=3.12.2\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.9 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3,>=0.28.1\n",
            "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.8.0-py2.py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 78.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.7.1-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 79.9 MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.13.5-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 73.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.13.4-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 80.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.13.3-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 76.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.13.2-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 76.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.13.1-py2.py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 81.5 MB/s \n",
            "\u001b[?25hCollecting grpcio-status>=1.16.0\n",
            "  Downloading grpcio_status-1.48.1-py3-none-any.whl (14 kB)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
            "  Downloading google_cloud_pubsublite-1.4.2-py2.py3-none-any.whl (265 kB)\n",
            "\u001b[K     |████████████████████████████████| 265 kB 80.5 MB/s \n",
            "\u001b[?25hCollecting overrides<7.0.0,>=6.0.1\n",
            "  Downloading overrides-6.2.0-py3-none-any.whl (17 kB)\n",
            "Collecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.7.0-py2.py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 69.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_recommendations_ai-0.6.2-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting grpcio<2,>=1.33.1\n",
            "  Downloading grpcio-1.48.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 42.0 MB/s \n",
            "\u001b[?25hCollecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.4.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.10.3.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (0.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir-eval>=0.4->magenta==2.1.0) (0.16.0)\n",
            "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
            "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.2->magenta==2.1.0) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.1.1)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.2->magenta==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (2.21)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.0) (1.14.1)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.0) (0.8.10)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.0) (0.1.7)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.0) (22.1.0)\n",
            "Collecting note-seq\n",
            "  Downloading note_seq-0.0.4-py3-none-any.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 76.8 MB/s \n",
            "\u001b[?25h  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.0) (1.3.5)\n",
            "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.0) (2.3.3)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.0) (7.9.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (6.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree>=2.1.0->note-seq->magenta==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.0.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->note-seq->magenta==2.1.0) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->note-seq->magenta==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->note-seq->magenta==2.1.0) (0.7.0)\n",
            "Collecting gevent\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (3.1.0)\n",
            "Collecting pypng\n",
            "  Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (0.5.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (1.12.11)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (4.64.0)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (4.6.0.66)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow\n",
            "  Downloading mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (0.25.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (1.1.4)\n",
            "Collecting tensorflow-probability\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.0) (1.7.1)\n",
            "Collecting tensorflow-gan\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 76.9 MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->tensor2tensor->magenta==2.1.0) (3.8.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->magenta==2.1.0) (1.1.3)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 74.3 MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.0) (3.0.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tensor2tensor->magenta==2.1.0) (1.5.2)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 71.4 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->magenta==2.1.0) (1.2.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (2.0.7)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (0.26.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (14.0.6)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (2.8.0)\n",
            "Collecting protobuf<4,>=3.12.2\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (1.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->magenta==2.1.0) (2.7.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.0) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.0) (5.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.0) (1.10.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor->magenta==2.1.0) (0.12.0)\n",
            "Building wheels for collected packages: dill, google-apitools, mir-eval, pretty-midi, pygtrie, python-rtmidi, docopt, bz2file\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=7273e831ae758132f96dc27bf59b9ad11f6f26d9f4f612e988daf33d758b553b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131039 sha256=f4e1bff9a5c18d194c4b071d63961e3bc55ab52de863f725174cbc828aa9ee8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100721 sha256=c3905aab5ea3fa4c7acca239186203c1717f91e8464f67709cdc1abbaca4e42f\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/5a/46/d2527ff1fd975e1a793375e6ed763bfe4d3ea396b7cdc470eb\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591955 sha256=5bd14d0b7b7593f58759ff573b567a05be46986b839c9390f40f1c6c10e41f27\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.5.0-py3-none-any.whl size=20944 sha256=c38679e0a2141e5288d5cd8ca19f68809becb1c74efc663e904e87702c14cdd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/5b/c8/7791f519f0b9f7186bd7531fe8749a849934739069ee2f2a43\n",
            "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=391715 sha256=8dfa12a8b71dc8a997acfb676d45da5fb8a1f2340710b5bf881a62f697b1c073\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/0c/e1/ccca9af1590f715067166c3199f6b639a26a152f61d4e79397\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=40793e6151e4445333a4a55f5e8487134823adad02c65d455bf21bb317d9a4dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=ad1c9403117db53cbb709afd101bba350d02e3626d123a49889fb96597031f46\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/ce/8d/b5f76b602b16a8a39f2ded74189cf5f09fc4a87bea16c54a8b\n",
            "Successfully built dill google-apitools mir-eval pretty-midi pygtrie python-rtmidi docopt bz2file\n",
            "Installing collected packages: protobuf, requests, grpcio, llvmlite, proto-plus, numba, grpcio-status, grpcio-gcp, grpc-google-iam-v1, docopt, cloudpickle, zope.interface, zope.event, tensorflow-probability, resampy, pymongo, overrides, orjson, mido, jedi, hdfs, google-cloud-pubsub, google-cloud-core, google-auth-httplib2, fasteners, fastavro, dill, tf-slim, tensorflow-gan, tensorflow-addons, pypng, pydub, pretty-midi, mesh-tensorflow, kfac, gunicorn, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-cloud-bigquery-storage, google-apitools, gevent, bz2file, apache-beam, tensor2tensor, sox, sk-video, python-rtmidi, pygtrie, note-seq, mir-eval, dm-sonnet, magenta\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.0\n",
            "    Uninstalling llvmlite-0.39.0:\n",
            "      Successfully uninstalled llvmlite-0.39.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.0\n",
            "    Uninstalling numba-0.56.0:\n",
            "      Successfully uninstalled numba-0.56.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.16.0\n",
            "    Uninstalling tensorflow-probability-0.16.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.16.0\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.4.0\n",
            "    Uninstalling resampy-0.4.0:\n",
            "      Successfully uninstalled resampy-0.4.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.2.0\n",
            "    Uninstalling pymongo-4.2.0:\n",
            "      Successfully uninstalled pymongo-4.2.0\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-auth-httplib2\n",
            "    Found existing installation: google-auth-httplib2 0.0.4\n",
            "    Uninstalling google-auth-httplib2-0.0.4:\n",
            "      Successfully uninstalled google-auth-httplib2-0.0.4\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: google-cloud-language\n",
            "    Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Attempting uninstall: google-cloud-bigquery-storage\n",
            "    Found existing installation: google-cloud-bigquery-storage 1.1.2\n",
            "    Uninstalling google-cloud-bigquery-storage-1.1.2:\n",
            "      Successfully uninstalled google-cloud-bigquery-storage-1.1.2\n",
            "Successfully installed apache-beam-2.41.0 bz2file-0.98 cloudpickle-2.1.0 dill-0.3.1.1 dm-sonnet-2.0.0 docopt-0.6.2 fastavro-1.6.0 fasteners-0.17.3 gevent-21.12.0 google-apitools-0.5.31 google-auth-httplib2-0.1.0 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.2 google-cloud-core-1.7.3 google-cloud-dlp-3.7.1 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.1 google-cloud-pubsublite-1.4.2 google-cloud-recommendations-ai-0.6.2 google-cloud-spanner-1.19.3 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 grpc-google-iam-v1-0.12.4 grpcio-1.48.1 grpcio-gcp-0.2.2 grpcio-status-1.48.1 gunicorn-20.1.0 hdfs-2.7.0 jedi-0.18.1 kfac-0.2.0 llvmlite-0.32.1 magenta-2.1.0 mesh-tensorflow-0.1.21 mido-1.2.6 mir-eval-0.7 note-seq-0.0.3 numba-0.49.1 orjson-3.8.0 overrides-6.2.0 pretty-midi-0.2.9 proto-plus-1.22.1 protobuf-3.19.4 pydub-0.25.1 pygtrie-2.5.0 pymongo-3.12.3 pypng-0.20220715.0 python-rtmidi-1.1.2 requests-2.28.1 resampy-0.3.1 sk-video-1.1.10 sox-1.4.1 tensor2tensor-1.15.7 tensorflow-addons-0.17.1 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Dependency issue 해결\n",
        "!pip install magenta==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import IPython\n",
        "import collections\n",
        "import note_seq \n",
        "from magenta.common import merge_hparams\n",
        "from magenta.contrib import training as contrib_training\n",
        "from magenta.models.music_vae import MusicVAE\n",
        "from magenta.models.music_vae import lstm_models\n",
        "from magenta.models.music_vae import data\n",
        "from magenta.scripts.convert_dir_to_note_sequences import convert_directory #전처리함수\n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae.trained_model import TrainedModel\n",
        "import tensorflow.compat.v1 as tf # tensorflow 1과 2의 호환성을 위해 진행 \n",
        "tf.disable_v2_behavior()\n",
        "import tf_slim "
      ],
      "metadata": {
        "id": "RV1X3ncSpYci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6237b9aa-4d13-4d78-9161-4a3c494c4bd6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 zip파일 불러오기\n",
        "url = \"https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\"\n",
        "dir = tf.keras.utils.get_file(origin=url, \n",
        "                                   fname='/content/groove.zip', \n",
        "                                   extract=True)\n",
        "data_dir = pathlib.Path(dir)"
      ],
      "metadata": {
        "id": "VnxmvrGvsVaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c174cc6-4f75-4ec9-bac8-1a74dc73d313"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n",
            "3260416/3260318 [==============================] - 0s 0us/step\n",
            "3268608/3260318 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zip파일 압축해제\n",
        "zipfile.ZipFile('/content/groove.zip').extractall()"
      ],
      "metadata": {
        "id": "tRPK4sp2sbf7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#경로지정\n",
        "data_root= '/content/groove'\n",
        "# midi파일의 session, id, bpm info확인을 위함\n",
        "csv_file = '/content/groove/info.csv' \n",
        "# tfrecord파일 경로를 지정\n",
        "tfrec_root = '/content/drum.tfrecord'"
      ],
      "metadata": {
        "id": "tUsBHDC2scUO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/groove/info.csv')\n",
        "df = pd.DataFrame(df)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "DI6b4GTEscR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "e960c4fb-b930-498f-8bab-40fa10723313"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    drummer                session                        id           style  \\\n",
              "0  drummer1  drummer1/eval_session   drummer1/eval_session/1    funk/groove1   \n",
              "1  drummer1  drummer1/eval_session  drummer1/eval_session/10   soul/groove10   \n",
              "2  drummer1  drummer1/eval_session   drummer1/eval_session/2    funk/groove2   \n",
              "3  drummer1  drummer1/eval_session   drummer1/eval_session/3    soul/groove3   \n",
              "4  drummer1  drummer1/eval_session   drummer1/eval_session/4    soul/groove4   \n",
              "5  drummer1  drummer1/eval_session   drummer1/eval_session/5    funk/groove5   \n",
              "6  drummer1  drummer1/eval_session   drummer1/eval_session/6  hiphop/groove6   \n",
              "7  drummer1  drummer1/eval_session   drummer1/eval_session/7     pop/groove7   \n",
              "8  drummer1  drummer1/eval_session   drummer1/eval_session/8    rock/groove8   \n",
              "9  drummer1  drummer1/eval_session   drummer1/eval_session/9    soul/groove9   \n",
              "\n",
              "   bpm beat_type time_signature  \\\n",
              "0  138      beat            4-4   \n",
              "1  102      beat            4-4   \n",
              "2  105      beat            4-4   \n",
              "3   86      beat            4-4   \n",
              "4   80      beat            4-4   \n",
              "5   84      beat            4-4   \n",
              "6   87      beat            4-4   \n",
              "7  138      beat            4-4   \n",
              "8   65      beat            4-4   \n",
              "9  105      beat            4-4   \n",
              "\n",
              "                                       midi_filename  \\\n",
              "0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n",
              "1  drummer1/eval_session/10_soul-groove10_102_bea...   \n",
              "2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n",
              "3  drummer1/eval_session/3_soul-groove3_86_beat_4...   \n",
              "4  drummer1/eval_session/4_soul-groove4_80_beat_4...   \n",
              "5  drummer1/eval_session/5_funk-groove5_84_beat_4...   \n",
              "6  drummer1/eval_session/6_hiphop-groove6_87_beat...   \n",
              "7  drummer1/eval_session/7_pop-groove7_138_beat_4...   \n",
              "8  drummer1/eval_session/8_rock-groove8_65_beat_4...   \n",
              "9  drummer1/eval_session/9_soul-groove9_105_beat_...   \n",
              "\n",
              "                                      audio_filename   duration split  \n",
              "0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n",
              "1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n",
              "2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  \n",
              "3  drummer1/eval_session/3_soul-groove3_86_beat_4...  44.716543  test  \n",
              "4  drummer1/eval_session/4_soul-groove4_80_beat_4...  47.987500  test  \n",
              "5  drummer1/eval_session/5_funk-groove5_84_beat_4...  45.687518  test  \n",
              "6  drummer1/eval_session/6_hiphop-groove6_87_beat...  44.119242  test  \n",
              "7  drummer1/eval_session/7_pop-groove7_138_beat_4...  27.706547  test  \n",
              "8  drummer1/eval_session/8_rock-groove8_65_beat_4...  59.067313  test  \n",
              "9  drummer1/eval_session/9_soul-groove9_105_beat_...  36.540504  test  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59005e30-7dd9-4198-9c11-3c12d446e66c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>drummer</th>\n",
              "      <th>session</th>\n",
              "      <th>id</th>\n",
              "      <th>style</th>\n",
              "      <th>bpm</th>\n",
              "      <th>beat_type</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>midi_filename</th>\n",
              "      <th>audio_filename</th>\n",
              "      <th>duration</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/1</td>\n",
              "      <td>funk/groove1</td>\n",
              "      <td>138</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
              "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
              "      <td>27.872308</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/10</td>\n",
              "      <td>soul/groove10</td>\n",
              "      <td>102</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
              "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
              "      <td>37.691158</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/2</td>\n",
              "      <td>funk/groove2</td>\n",
              "      <td>105</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
              "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
              "      <td>36.351218</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/3</td>\n",
              "      <td>soul/groove3</td>\n",
              "      <td>86</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
              "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
              "      <td>44.716543</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/4</td>\n",
              "      <td>soul/groove4</td>\n",
              "      <td>80</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
              "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
              "      <td>47.987500</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/5</td>\n",
              "      <td>funk/groove5</td>\n",
              "      <td>84</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/5_funk-groove5_84_beat_4...</td>\n",
              "      <td>drummer1/eval_session/5_funk-groove5_84_beat_4...</td>\n",
              "      <td>45.687518</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/6</td>\n",
              "      <td>hiphop/groove6</td>\n",
              "      <td>87</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/6_hiphop-groove6_87_beat...</td>\n",
              "      <td>drummer1/eval_session/6_hiphop-groove6_87_beat...</td>\n",
              "      <td>44.119242</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/7</td>\n",
              "      <td>pop/groove7</td>\n",
              "      <td>138</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/7_pop-groove7_138_beat_4...</td>\n",
              "      <td>drummer1/eval_session/7_pop-groove7_138_beat_4...</td>\n",
              "      <td>27.706547</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/8</td>\n",
              "      <td>rock/groove8</td>\n",
              "      <td>65</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/8_rock-groove8_65_beat_4...</td>\n",
              "      <td>drummer1/eval_session/8_rock-groove8_65_beat_4...</td>\n",
              "      <td>59.067313</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>drummer1</td>\n",
              "      <td>drummer1/eval_session</td>\n",
              "      <td>drummer1/eval_session/9</td>\n",
              "      <td>soul/groove9</td>\n",
              "      <td>105</td>\n",
              "      <td>beat</td>\n",
              "      <td>4-4</td>\n",
              "      <td>drummer1/eval_session/9_soul-groove9_105_beat_...</td>\n",
              "      <td>drummer1/eval_session/9_soul-groove9_105_beat_...</td>\n",
              "      <td>36.540504</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59005e30-7dd9-4198-9c11-3c12d446e66c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59005e30-7dd9-4198-9c11-3c12d446e66c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59005e30-7dd9-4198-9c11-3c12d446e66c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "midi데이터를 학습하기위해, 백터화 해야하며, 백터화하여 TFrecord로 저장할 것 입니다.\n",
        "midi 포멧으로 읽어서 매번 디코딩을 진행하면 학습단계에서 데이터를 읽을때 성능저하가 발생합니다. 이를 해결하기위해 TFrecord파일을 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "fWByBOzTqUI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_directory(data_root,tfrec_root,recursive=True) #전처리 함수"
      ],
      "metadata": {
        "id": "G9eoTIGCscPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51c15e8-f269-4140-83a1-f2f962780ee1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/info.csv\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/README\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/LICENSE\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer10/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer10/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer6/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer2/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer3/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer9/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer9/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer1/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer8/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer5/eval_session/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer4/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer4/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session3/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session2/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/session1/Icon\n",
            "WARNING:tensorflow:Unable to find a converter for file /content/groove/drummer7/eval_session/Icon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configs.py 26-46, 500-523 부분을 가져와 선언\n",
        "\n",
        "class Config(collections.namedtuple(\n",
        "    'Config',\n",
        "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
        "     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
        "\n",
        "    def values(self):\n",
        "        return self._asdict()\n",
        "\n",
        "Config.__new__.__defaults__ = (None,) * len(Config._fields)\n",
        "\n",
        "HParams = contrib_training.HParams\n",
        "\n",
        "def update_config(config, update_dict):\n",
        "    config_dict = config.values()\n",
        "    config_dict.update(update_dict)\n",
        "    return Config(**config_dict)\n",
        "\n",
        "\n",
        "CONFIG_MAP = {}\n",
        "\n",
        "# 모델 config configs.py의 groovae config 사용\n",
        "\n",
        "#=== 모델구조 ===\n",
        "CONFIG_MAP['groovae_4bar'] = Config(\n",
        "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(), #BidirectionalLstmEncoder 사용 \n",
        "                   lstm_models.GrooveLstmDecoder()), #Hierarchical Decoder\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            batch_size=512, #데이터 배치사이즈\n",
        "            max_seq_len=16 * 4,  # 4마디 길이지정\n",
        "            z_size=256, #잠재백터 사이즈\n",
        "            enc_rnn_size=[512], #인코더 순환 사이즈지정\n",
        "            dec_rnn_size=[256, 256],#디코더 순환사이즈 지정\n",
        "            max_beta=0.2,\n",
        "            free_bits=48,\n",
        "            dropout_keep_prob=0.3,#드롭아웃\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.GrooveConverter(\n",
        "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n",
        "        max_tensors_per_notesequence=20,\n",
        "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
        "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
        "    # tfds_name='groove/4bar-midionly',\n",
        "    train_examples_path='/content/drum.tfrecord', #데이터 경로 설정\n",
        ")"
      ],
      "metadata": {
        "id": "eyDlM5E6scMG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "TFRecord 데이터를 입력 시퀀스로 하여 VAE 모델을 학습시킬 수 있습니다. MusicVAE의 configs.py에는 다양한 설정이 있고, Groove MIDI Dataset으로 MusicVAE 모델을 학습시키고 4마디에 해당하는 드럼 샘플을 생성하는 과정입니다."
      ],
      "metadata": {
        "id": "w7xCDhcupQkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# music_vae_train.py 초기 flag를 제외하고 _trial_summary, get_input_tensors, train 함수를 가져온다.\n",
        "# Should not be called from within the graph to avoid redundant summaries.\n",
        "def _trial_summary(hparams, examples_path, output_dir):\n",
        "  #텐서보드 summary 텍스트\n",
        "\n",
        "    examples_path_summary = tf.summary.text(\n",
        "      'examples_path', tf.constant(examples_path, name='examples_path'),\n",
        "      collections=[])\n",
        "\n",
        "    hparams_dict = hparams.values()\n",
        "\n",
        "#하이퍼 파라미터\n",
        "  # Create a markdown table from hparams.\n",
        "    header = '| Key | Value |\\n| :--- | :--- |\\n'\n",
        "    keys = sorted(hparams_dict.keys())\n",
        "    lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
        "    hparams_table = header + '\\n'.join(lines) + '\\n'\n",
        "\n",
        "    hparam_summary = tf.summary.text(\n",
        "      'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
        "        writer.add_summary(examples_path_summary.eval())\n",
        "        writer.add_summary(hparam_summary.eval())\n",
        "        writer.close()\n",
        "\n",
        "\n",
        "def _get_input_tensors(dataset, config):\n",
        "  #데이터로부터 텐서 입력 \n",
        "    batch_size = config.hparams.batch_size\n",
        "    iterator = tf.data.make_one_shot_iterator(dataset)\n",
        "    (input_sequence, output_sequence, control_sequence,\n",
        "    sequence_length) = iterator.get_next()\n",
        "    input_sequence.set_shape(\n",
        "    [batch_size, None, config.data_converter.input_depth])\n",
        "    output_sequence.set_shape(\n",
        "    [batch_size, None, config.data_converter.output_depth])\n",
        "    \n",
        "    if not config.data_converter.control_depth:\n",
        "        control_sequence = None\n",
        "    \n",
        "    else:\n",
        "        control_sequence.set_shape(\n",
        "            [batch_size, None, config.data_converter.control_depth])\n",
        "        sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
        "        \n",
        "    return {\n",
        "        'input_sequence': input_sequence,\n",
        "        'output_sequence': output_sequence,\n",
        "        'control_sequence': control_sequence,\n",
        "        'sequence_length': sequence_length\n",
        "    }\n",
        "\n",
        "#훈련 체크포인트\n",
        "def train(train_dir,\n",
        "          config,\n",
        "          dataset_fn,\n",
        "          checkpoints_to_keep=5,\n",
        "          keep_checkpoint_every_n_hours=1,\n",
        "          num_steps=None,\n",
        "          master='',\n",
        "          num_sync_workers=0,\n",
        "          num_ps_tasks=0,\n",
        "          task=0):\n",
        "\n",
        "# train\n",
        "    tf.gfile.MakeDirs(train_dir)\n",
        "    is_chief = (task == 0)\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.device(tf.train.replica_device_setter(\n",
        "            num_ps_tasks, merge_devices=True)):\n",
        "            \n",
        "            model = config.model\n",
        "            model.build(config.hparams,\n",
        "                        config.data_converter.output_depth,\n",
        "                        is_training=True)\n",
        "            #옵티마이저\n",
        "            optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
        "\n",
        "            hooks = []\n",
        "            if num_sync_workers:\n",
        "                optimizer = tf.train.SyncReplicasOptimizer(\n",
        "                    optimizer,num_sync_workers)\n",
        "                hooks.append(optimizer.make_session_run_hook(is_chief))\n",
        "\n",
        "            grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
        "            global_norm = tf.global_norm(grads)\n",
        "            tf.summary.scalar('global_norm', global_norm)\n",
        "            \n",
        "            if config.hparams.clip_mode == 'value':\n",
        "                g = config.hparams.grad_clip\n",
        "                clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
        "            elif config.hparams.clip_mode == 'global_norm':\n",
        "                clipped_grads = tf.cond(\n",
        "                    global_norm < config.hparams.grad_norm_clip_to_zero,\n",
        "                    lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
        "                        grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
        "                    lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                list(zip(clipped_grads, var_list)),\n",
        "                global_step=model.global_step,\n",
        "                name='train_step')\n",
        "            logging_dict = {'global_step': model.global_step,\n",
        "                            'loss': model.loss}\n",
        "            \n",
        "            hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
        "            if num_steps:\n",
        "                hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
        "                \n",
        "            scaffold = tf.train.Scaffold(\n",
        "                saver=tf.train.Saver(\n",
        "                    max_to_keep=checkpoints_to_keep,\n",
        "                    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
        "            \n",
        "            tf_slim.training.train(\n",
        "                train_op = train_op,\n",
        "                logdir=train_dir,\n",
        "                scaffold=scaffold,\n",
        "                hooks=hooks,\n",
        "                save_checkpoint_secs=60,\n",
        "                master=master,\n",
        "                is_chief=is_chief)\n",
        "\n",
        "\n",
        "\n",
        "def run(config_map,\n",
        "        tf_file_reader=tf.data.TFRecordDataset,\n",
        "        file_reader=tf.compat.v1.python_io.tf_record_iterator,\n",
        "        is_training=True):\n",
        "    config = config_map['groovae_4bar']\n",
        "    train_dir = '/content/train'\n",
        "    num_steps = 10000 #훈련 epoch\n",
        "    \n",
        "    def dataset_fn():\n",
        "        return data.get_dataset(\n",
        "            config,\n",
        "            tf_file_reader=tf_file_reader,\n",
        "            is_training=True,\n",
        "            cache_dataset=True)\n",
        "    \n",
        "    if is_training == True:\n",
        "        train(\n",
        "            train_dir,\n",
        "            config=config,\n",
        "            dataset_fn=dataset_fn,\n",
        "            num_steps=num_steps)      \n",
        "    \n",
        "    else:\n",
        "        print(\"EVAL\")"
      ],
      "metadata": {
        "id": "_gCCSPhlaFrn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(CONFIG_MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJO5RUQKeSFv",
        "outputId": "96c7b884-5521-4976-d015-4897f2d218c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/magenta/contrib/rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
            "/usr/local/lib/python3.7/dist-packages/magenta/contrib/rnn.py:754: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.constant_initializer(0.0))\n",
            "/usr/local/lib/python3.7/dist-packages/magenta/models/music_vae/base_model.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/magenta/models/music_vae/base_model.py:205: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
            "/usr/local/lib/python3.7/dist-packages/magenta/models/music_vae/lstm_utils.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  name=name),\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1054: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련된 체크포인트를 이용해서 새로운 MIDI 데이터를 생성할 수 있습니다. 생성 방식에는 ‘Sample’ 방식과 ‘Interpolate’ 방식이 있습니다."
      ],
      "metadata": {
        "id": "g3CnyQPhppD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#music_vae/music_vae_generate.py\n",
        "model = TrainedModel(\n",
        "    config=CONFIG_MAP['groovae_4bar'],\n",
        "    batch_size=1,\n",
        "    checkpoint_dir_or_path='/content/train')#checkpoint\n",
        "\n",
        "generated_sequence = model.sample(n=1, length=16*4, temperature=0.5)\n",
        "note_seq.sequence_proto_to_midi_file(generated_sequence[0], '/content/drum_4bar.mid')"
      ],
      "metadata": {
        "id": "-WNuB7MnyKi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urudxVbCjqLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
